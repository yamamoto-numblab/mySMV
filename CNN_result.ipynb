{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchsummary import summary\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#win\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#Mac\n",
    "# device = torch.device('mps')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainAdress = ['Fp1','AF7','AF3','F1','F3','F5','F7','FT7','FC5',\n",
    "'FC3','FC1','C1','C3','C5','T7','TP7','CP5','CP3','CP1','P1','P3',\n",
    "'P5','P7','P9','PO7','PO3','O1','Iz','Oz','POz','Pz','CPz','Fpz','Fp2',\n",
    "'AF8','AF4','AFz','Fz','F2','F4','F6','F8','FT8','FC6','FC4','FC2',\n",
    "'FCz','Cz','C2','C4','C6','T8','TP8','CP6','CP4','CP2','P2','P4',\n",
    "'P6','P8','P10','PO8','PO4','O2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy dataの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day1_data = np.load('numpy_data/subject1_data1.npy')\n",
    "day1_label = np.load('numpy_data/subject1_label1.npy')\n",
    "day2_data = np.load('numpy_data/subject1_data2.npy')\n",
    "day2_label = np.load('numpy_data/subject1_label2.npy')\n",
    "\n",
    "# day1_data2 = np.load('numpy_data/subject2_data1.npy')\n",
    "# day1_label2 = np.load('numpy_data/subject2_label1.npy')\n",
    "# day2_data2 = np.load('numpy_data/subject2_data2.npy')\n",
    "# day2_label2 = np.load('numpy_data/subject2_label2.npy')\n",
    "\n",
    "# day1_data3 = np.load('numpy_data/subject3_data1.npy')\n",
    "# day1_label3 = np.load('numpy_data/subject3_label1.npy')\n",
    "# day2_data3 = np.load('numpy_data/subject3_data2.npy')\n",
    "# day2_label3 = np.load('numpy_data/subject3_label2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_data = np.vstack([day1_data,day2_data])\n",
    "label_data = np.hstack([day1_label,day2_label])\n",
    "# brain_data = np.vstack([day1_data,day2_data,day1_data2,day2_data2,day1_data3,day2_data3])\n",
    "# label_data = np.hstack([day1_label,day2_label,day1_label2,day2_label2,day1_label2,day2_label2])\n",
    "# info_motorbrain = ['FC3','FC1','C1','C3','C5','CP3','CP1','CPz','FC4','FC2',\n",
    "# 'FCz','Cz','C2','C4','C6','CP4','CP2',]\n",
    "# info_motor = mne.create_info(ch_names=info_motorbrain, ch_types=\"eeg\", sfreq=1024)\n",
    "# info_motor.set_montage('standard_1020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#運動野領域のデータ　sampling rate は1024->100 Hz タスクは4秒間\n",
    "motor_brainAdress = [9,10,11,12,13,17,18,31,44,45,46,47,48,49,50,54,55]\n",
    "data_numpy_task  = brain_data[:,:,100*6:100*10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"brain_data\",np.shape(data_numpy_task))\n",
    "print(\"label\",np.shape(label_data))\n",
    "label_data = label_data -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train testの分割\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data_numpy_task,label_data, test_size=0.15,random_state=42)\n",
    "print(np.shape(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(np.expand_dims(X_train, axis=1))\n",
    "X_test = torch.from_numpy(np.expand_dims(X_test, axis=1))\n",
    "label = torch.tensor(Y_train)\n",
    "label2 = torch.tensor(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape, label.shape)\n",
    "print(X_test.shape, label2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasetを作成\n",
    "Dataset = torch.utils.data.TensorDataset(x_train, label)\n",
    "# Datasetを作成\n",
    "tast_data = torch.utils.data.TensorDataset(X_test, label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "Learning_Rate = 0.001\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(dataset=Dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(dataset=tast_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEGNet using Pytorch\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, AF=nn.ELU(alpha=1)):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.firstConv = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, (1, 51), stride=(1, 1), padding=(0, 25), bias=False),\n",
    "            nn.BatchNorm2d(16, 1e-05)\n",
    "        )      \n",
    "        self.depthwiseConv = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, (2, 1), stride=(1, 1), groups=16, bias=False),\n",
    "            nn.BatchNorm2d(32, 1e-05),\n",
    "            AF,\n",
    "            nn.AvgPool2d(kernel_size=(1,4), stride=(1,4), padding=0),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.separableConv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, (1, 15), stride=(1, 1), padding=(0, 7), bias=False),\n",
    "            nn.BatchNorm2d(32, 1e-05),\n",
    "            AF,\n",
    "            nn.AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0),\n",
    "            nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.classify = nn.Sequential(\n",
    "            nn.Linear(in_features=24192, out_features=2, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.firstConv(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.separableConv(x)\n",
    "        x = self.classify(x.view(len(x), -1))\n",
    "        return x    \n",
    "EEGNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, model=None, lr=0.001):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = model\n",
    "        self.losses = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "        \n",
    "    def fit(self, trainloader=None, validloader=None, epochs=1, monitor=None, only_print_finish_ep_num=False):\n",
    "        doValid = False if validloader == None else True\n",
    "        pre_ck_point = [float(\"inf\"), 0.0, float(\"inf\"), 0.0, 0] # loss, acc, val_loss, val_acc, epoch\n",
    "        history = {\"loss\": [], \"acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "        for ep in range(1, epochs + 1):\n",
    "            proc_start = time.time() # timer start\n",
    "            if (not (ep % 10)) or (ep == 1):\n",
    "                if not only_print_finish_ep_num:\n",
    "                    print(f\"Epoch {ep}/{epochs}\")\n",
    "            self.model.train()       # Train mode\n",
    "            step = 1                 # Restart step\n",
    "            for x_batch, y_batch in trainloader:\n",
    "                x_batch, y_batch = x_batch.to(device, dtype=torch.float), y_batch.to(device)\n",
    "                pred = self.model(x_batch)\n",
    "                loss = self.losses(pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                if (not (ep % 10)) or (ep == 1):\n",
    "                    pbar = int(step * 30 / len(trainloader))\n",
    "                    if not only_print_finish_ep_num:\n",
    "                        print(\"\\r{}/{} [{}{}]\".format(\n",
    "                            step, len(trainloader), \">\" * pbar, \" \" * (30 - pbar)), \n",
    "                            end=\"\")\n",
    "                step += 1\n",
    "            loss, acc = self.evaluate(trainloader)   # Loss & Accuracy\n",
    "            val_loss, val_acc = self.evaluate(validloader) if doValid else (0, 0)   # if have validation dataset, evaluate validation\n",
    "            history[\"loss\"] = np.append(history[\"loss\"], loss)\n",
    "            history[\"acc\"] = np.append(history[\"acc\"], acc)\n",
    "            history[\"val_loss\"] = np.append(history[\"val_loss\"], val_loss)\n",
    "            history[\"val_acc\"] = np.append(history[\"val_acc\"], val_acc)\n",
    "            # Update checkpoint\n",
    "            if self.__updateCheckpoint(monitor, pre_ck_point, [loss, acc, val_loss, val_acc, ep]):\n",
    "                save_file_name = f\"checkpoint_model_ep-{ep}.pt\"\n",
    "                self.save(\"./data/\"+save_file_name)\n",
    "                pre_ck_point = [loss, acc, val_loss, val_acc, ep]\n",
    "                history['lastest_model_path'] = save_file_name\n",
    "                \n",
    "            if only_print_finish_ep_num and (ep % 50 == 0):\n",
    "                print(f\"{ep} \", end=\" \")\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        total, acc = 0, 0\n",
    "        self.model.eval()           # Eval mode\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device, dtype=torch.float), y_batch.to(device)\n",
    "            pred = self.model(x_batch)\n",
    "            loss = self.losses(pred, y_batch).item()\n",
    "            total += y_batch.shape[0]     # Number of data\n",
    "            acc += (torch.sum(pred.argmax(dim=1)==y_batch)).item()     # Sum the prediction that's correct\n",
    "        acc /= total     # Accuracy = correct prediction / number of data\n",
    "        return (loss, acc)\n",
    "    \n",
    "    def predict(self, dataset):\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)\n",
    "        prediction = []\n",
    "        truth = []\n",
    "        self.model.eval()\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device, dtype=torch.float), y_batch.to(device)\n",
    "            pred = self.model(x_batch).cpu()\n",
    "            prediction = np.append(prediction, pred.argmax(dim=1).numpy())\n",
    "            truth = np.append(truth, y_batch.cpu().numpy())            \n",
    "        return prediction, truth\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save(self.model, filepath)\n",
    "        \n",
    "    #@classmethod\n",
    "    def load(cls, filepath):\n",
    "        return cls(torch.load(filepath))\n",
    "    \n",
    "    def __updateCheckpoint(self, monitor, pre_ck_point, evaluation):\n",
    "        if type(monitor) is int:\n",
    "            return True if evaluation[4] % monitor == 0 else False\n",
    "        elif type(monitor) is list:\n",
    "            for _ in monitor:\n",
    "                if not _ in [\"loss\", \"acc\", \"val_loss\", \"val_acc\"]:\n",
    "                    raise Exception(f\"\\\"{_}\\\" is not a valid monitor condition.\")\n",
    "                elif _ == \"loss\" and pre_ck_point[0] <= evaluation[0]:\n",
    "                    return False # present epoch loss > history loss\n",
    "                elif _ == \"acc\" and pre_ck_point[1] >= evaluation[1]:\n",
    "                    return False # present epoch acc <= history acc\n",
    "                elif _ == \"val_loss\" and pre_ck_point[2] <= evaluation[2]:\n",
    "                    return False # present epoch val_loss > history val_loss\n",
    "                elif _ == \"val_acc\" and pre_ck_point[3] >= evaluation[3]:\n",
    "                    return False # present epoch val_acc < history val_acc        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 570/1000\n",
      "7/7 [>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]"
     ]
    }
   ],
   "source": [
    "eegnet = EEGNet().to(device)\n",
    "summary(eegnet, (1, 64, 400))\n",
    "model = Model(eegnet, lr=Learning_Rate)\n",
    "history = model.fit(trainloader=trainloader, validloader=testloader, epochs=EPOCHS, monitor=[\"acc\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_and_loss(history, figsize=(10,4), base_save_path=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    if base_save_path:\n",
    "        st = fig.suptitle(base_save_path, fontsize=\"x-large\")\n",
    "    \n",
    "    ax1.title.set_text(\"Acc\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    l1 = ax1.plot(history[\"acc\"], color=\"red\", label='train')\n",
    "    l2 = ax1.plot(history[\"val_acc\"], color=\"blue\", label='test')\n",
    "    \n",
    "    ax2.title.set_text(\"Loss\")\n",
    "    ax2.set_ylabel(\"Epochs\")\n",
    "    l3 = ax2.plot(history[\"loss\"], color=\"red\", label='train')\n",
    "    l4 = ax2.plot(history[\"val_loss\"], color=\"blue\", label='test')\n",
    "\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_and_loss(history=history, base_save_path=\"part_1.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68330a0c3a73c8a9ab17a58db04a4bc84979ee9d5927a73e0795658af7e94545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
