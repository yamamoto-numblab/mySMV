{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataクラス\n",
    "class O_Data:\n",
    "    def __init__(self, eeg, index, label):\n",
    "        self.eeg = eeg\n",
    "        self.index = index\n",
    "        self.label = label\n",
    "        self.stack = []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.stack.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_data65 = list()\n",
    "R_data = list()  #  実運動のデータを格納\n",
    "RI_data = list()  # 実運動＋想起運動のデータを格納\n",
    "All_data = list()  # 全データを格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Standardization(data): #標準化\n",
    "    after_data = scipy.stats.zscore(data)\n",
    "    b = np.average(after_data)\n",
    "    c = np.var(after_data)\n",
    "    return after_data\n",
    "\n",
    "def resampleing(data, fs1, fs2):\n",
    "    sec = len(data) / fs1\n",
    "    n = int(fs2 * sec)\n",
    "    resample_data = signal.resample(data, n)  # アンチエイリアス・リサンプリング\n",
    "    return resample_data\n",
    "\n",
    "def butter_lowpass(cutfreq, fs, order=4):\n",
    "    nyq = fs / 2.0\n",
    "    low = cutfreq / nyq\n",
    "    b, a = signal.butter(order, low, btype='low', fs=fs, output='ba')\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filt(data, cutfreq, fs, order=4):\n",
    "    b, a = butter_lowpass(cutfreq, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# FFT変換\n",
    "def FFT(data):\n",
    "    F = np.fft.fft(data)\n",
    "    F = F/max(F)\n",
    "    F = abs(F)\n",
    "    return F\n",
    "\n",
    "# 積分\n",
    "def calc_integral(start, stop, x, y):\n",
    "    sum = 0.0\n",
    "    j = start\n",
    "    for i in range(start, stop):\n",
    "        sum = sum + (y[j] + y[j + 1]) * (x[j + 1] - x[j]) / 2\n",
    "        j += 1\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filelist\n",
      "[' EEG_grasping_22-Nov-2022_sub1-1.mat', ' EEG_grasping_22-Nov-2022_sub1-2.mat', ' EEG_grasping_22-Nov-2022_sub1-3.mat', ' EEG_grasping_22-Nov-2022_sub2-2.mat', ' EEG_grasping_22-Nov-2022_sub2-3.mat', ' EEG_grasping_22-Nov-2022_sub2-1.mat']\n"
     ]
    }
   ],
   "source": [
    "filePath = filePath = '/Users/yamamotokouhei/subjectsData/data20221122'\n",
    "file_name = list()  # すべての.matファイルの名前\n",
    "for file in os.listdir(filePath):\n",
    "    base, ext = os.path.splitext(file)\n",
    "    if ext == '.mat':\n",
    "        file_name = sorted(file_name)\n",
    "        file_name.append(file)\n",
    "print('Filelist')\n",
    "file_name = [file_name[0],file_name[1],file_name[2],file_name[3],file_name[4],file_name[5]]\n",
    "file_num = len(file_name)\n",
    "print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loading\n"
     ]
    }
   ],
   "source": [
    "# 変数など\n",
    "data_directry = ''\n",
    "file_num = len(file_name)\n",
    "trial_num = 30\n",
    "all_trial = file_num * trial_num\n",
    "all_d = 0\n",
    "CH = 64\n",
    "\n",
    "A = 0  # トライアルカウント\n",
    "B = 0  # セッションカウント\n",
    "for s in range(6):\n",
    "    Dictionary = scipy.io.loadmat(filePath+'/'+file_name[s])\n",
    "    for t in range(trial_num):\n",
    "        for i, key in enumerate(Dictionary.keys()):\n",
    "            if i > 2:\n",
    "                a = Dictionary[key]\n",
    "                b = a[0, t][0][0]\n",
    "                eeg = b[0][0:64, :]\n",
    "                index = b[1]\n",
    "                samplerate = b[2]\n",
    "                # if s < 3:\n",
    "                #     label = 1\n",
    "\n",
    "                # else:\n",
    "                #     label = 2\n",
    "\n",
    "                label = b[5][0, 0]\n",
    "\n",
    "                o_data = O_Data(eeg, index, label)\n",
    "                o_data65.append(o_data)\n",
    "\n",
    "R_num = 0\n",
    "RI_num = 0\n",
    "\n",
    "print(\"data_loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = list()\n",
    "for n in range(all_trial):  # all_trial\n",
    "    number = list()  # restのindex\n",
    "    time = list()\n",
    "    all_d = sum(len(v) for v in o_data65[n].index) - 1\n",
    "    for i in range(all_d + 1):\n",
    "        time.append(i)\n",
    "    eeg = o_data65[n].eeg  # トライアルnのeeg\n",
    "    index = o_data65[n].index  # トライアルnのindex\n",
    "    label = o_data65[n].label  # トライアルnのlabel\n",
    "    for i in range(all_d):\n",
    "        if index[:, i] == 0:\n",
    "            number.append(i)\n",
    "    rest_time = len(number)  # rest時間のindex数\n",
    "\n",
    "\n",
    "\n",
    "    # すべてのチャンネルで行う\n",
    "\n",
    "\n",
    "    ave = 0\n",
    "    for j in range(64):\n",
    "        onech_eeg = eeg[j, :]  # 一つのチャンネルのeeg\n",
    "        number = np.array(number)\n",
    "        time = np.array(time)\n",
    "        filtered = signal.firwin(numtaps=51, cutoff=[5, 50], fs=1024, pass_zero=False)\n",
    "        y1 = signal.lfilter(filtered, 1, onech_eeg)\n",
    "        detrend = signal.detrend(y1)  # ベースライン補正\n",
    "        detrend_128 = resampleing(detrend, 1024, 128)  # ダウンサンプリング\n",
    "        standard = Standardization(detrend_128)[128*4:]  # 標準・2秒分(rest)カット\n",
    "        max_data = len(standard)\n",
    "        #F1 = np.fft.fft(standard)\n",
    "\n",
    "        ave += standard\n",
    "        F1 = np.fft.fft(ave[:750]/64)\n",
    "        amplitude1 = np.abs(F1)\n",
    "        F_amplitude1 = amplitude1 / 750 * 2\n",
    "        F_amplitude1[0] = F_amplitude1[0] / 2\n",
    "        #print(np.shape(F_amplitude1))\n",
    "        freq = np.fft.fftfreq(750,d = 1/128)\n",
    "        #print(freq)\n",
    "        pasa = 0\n",
    "        for l in range(32):\n",
    "            pasa += F_amplitude1[l+47]\n",
    "        All_data.append(standard)    \n",
    "   \n",
    "        if label == 1:\n",
    "                R_data.append(1)\n",
    "\n",
    "\n",
    "        else:\n",
    "                R_data.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11520,)\n",
      "(11520,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/7jpldjt16fq448x5ycffk_3c0000gn/T/ipykernel_9530/4141145904.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  All_data = np.array(All_data)\n"
     ]
    }
   ],
   "source": [
    "R_data = np.array(R_data)\n",
    "All_data = np.array(All_data)\n",
    "\n",
    "print(np.shape(R_data))\n",
    "print(np.shape(All_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lc/7jpldjt16fq448x5ycffk_3c0000gn/T/ipykernel_9530/3931677670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m SVC(C=1.0, class_weight=None, coef0=0.0,\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1072\u001b[0m         )\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1075\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(All_data, R_data, test_size=0.25)\n",
    "\n",
    "\n",
    "\n",
    "model =SVC(kernel='linear')\n",
    "model.fit(X_train,Y_train)\n",
    "\n",
    "SVC(C=1.0, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    "\n",
    "print(model.score(X_train, Y_train))\n",
    "print(model.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.640625\n",
      "{'C': 0.6, 'gamma': 0.6, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.38      0.51      1459\n",
      "           2       0.59      0.90      0.71      1421\n",
      "\n",
      "    accuracy                           0.64      2880\n",
      "   macro avg       0.69      0.64      0.61      2880\n",
      "weighted avg       0.70      0.64      0.61      2880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"C\":np.arange(0.1,1,0.5),\n",
    "    \"kernel\":[\"linear\",\"rbf\"],\n",
    "    \"gamma\" : np.arange(0.1,1,0.5)\n",
    "}\n",
    "grid = GridSearchCV(model, params,scoring=\"accuracy\", cv=5)\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "pred = grid.predict(X_test)\n",
    "print(classification_report(Y_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
